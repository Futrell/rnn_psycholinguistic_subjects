---
title: "rnnpsycholing Garden Pathing Verb Ambiguity II"
output:
  pdf_document: default
  html_notebook: default
---

We are looking for a 2x2 interaction of relative clause verb ambiguity and RC reduction:

The woman (who was) brought/given a sandwich from the kitchen tripped on the carpet

Why is this interesting?
(1) An interaction of reduction and verb ambiguity at the region Disambiguator shows that the network chose the more likely syntactic parse. 
(2) An interaction in the region End indicates that the disambiguating word was not sufficient to eliminate the representation of the earlier verb as a matrix verb, resulting in a context representation is less informative. 

Does the NN behave more like it has a minimal commitment theory or a garden path theory?

```{r}
rm(list = ls())
library(tidyverse)
library(lme4)
library(lmerTest)
library(plotrix)

REGION_ORDER = c("Start", "Noun", "Unreduced content", "RC Verb", "RC contents", "Disambiguator", "End")
REGION_EXEMPLARS = c("The", "woman", "who was", "brought/given", "the sandwich from the kitchen", "tripped", "on the carpet")
NUM_REGIONS = length(REGION_ORDER)

rename_LSTM = function(d) {
  d %>% mutate(LSTM=if_else(LSTM == "gulordava", "GRNN", if_else(LSTM == "google", "JRNN", LSTM)))
}

add_numeric_predictors <- function(d) {
  d %>%
    mutate(ambiguity.numeric=if_else(ambiguity=="unambig",-1,1)) %>%
    mutate(reduced.numeric=if_else(reduced=="reduced",1,-1))
}

d = read_csv("tests/combined_results.csv") %>%
  select(-X1) %>%
  separate(condition, sep="_", into=c("ambiguity", "reduced")) %>%
  mutate(region=if_else(region == "Unambiguous verb" | region == "Ambiguous verb", "RC Verb", region),
         region=factor(region, levels=REGION_ORDER)) %>%
  rename(LSTM=model)


d_agg = d %>% 
  rename_LSTM() %>%
  filter(region != "Start") %>%  # Aggregate surprisal by region
  group_by(sent_index, region, ambiguity, reduced, LSTM) %>% 
    summarise(surprisal=sum(surprisal),
              unk=any(unk)) %>%
    ungroup() %>% 
  filter(!unk) %>%
  mutate(reduced=factor(reduced, levels=c("unreduced", "reduced")), # Establish factor orders for dummy coding
         ambiguity=factor(ambiguity, levels=c("unambig", "ambig")))

```

## Overall visualization

```{r}

d_by_region = d_agg %>% 
  filter(region != "Start") %>%
  group_by(LSTM, region, sent_index) %>%
    mutate(item_mean=mean(surprisal)) %>%
    ungroup() %>%
  group_by(LSTM, region, ambiguity, reduced) %>%
    summarise(m=mean(surprisal),
              s=std.error(surprisal-item_mean),
              upper=m + 1.96*s,
              lower=m - 1.96*s) %>%
    ungroup() 

d_by_region %>%
  mutate(region=as.numeric(region)) %>% 
  ggplot(aes(x=region, y=m, ymax=upper, ymin=lower, linetype=ambiguity, color=reduced)) +
    geom_line() +
    geom_errorbar(linetype="solid", width=.1) +
    scale_x_continuous(breaks=seq(1, NUM_REGIONS), labels=REGION_EXEMPLARS) +
    theme(axis.text.x = element_text(angle=15, hjust=1)) +
    xlab("") +
    ylab("Sum surprisal in region") +
    facet_grid(LSTM~., scale="free_y") +
    theme(legend.position="top")

ggsave("verb_gardenpath.pdf", width=6, height=4)
  
```

The overall pattern we would infer from this plot of surprisal is: "tripped" is easy when the RC verb was preceded by "who was", making it unambiguous. It is harder when the RC is reduced, and even harder when the RC verb is ambiguous. The unambiguous RC verb appears to send a noisy signal that we are in an RC.

# Preregistered regressions

## Surprisal in the disambiguating region.

```{r}
d2 = d_agg %>% 
  add_numeric_predictors %>%
  filter(region == "Disambiguator")

mj = lmer(surprisal ~ ambiguity.numeric * reduced.numeric + (ambiguity.numeric+reduced.numeric|sent_index), data=filter(d2, LSTM == "JRNN"))
summary(mj)

mg = lmer(surprisal ~ ambiguity.numeric * reduced.numeric + (ambiguity.numeric+reduced.numeric|sent_index), data=filter(d2, LSTM == "GRNN"))
summary(mg)

m.aov = aov(surprisal ~ ambiguity * reduced + Error(as.factor(sent_index)/ambiguity*reduced), data=filter(d2, LSTM == "JRNN"))
summary(m.aov)

mj.reducedonly = aov(surprisal ~ ambiguity + Error(as.factor(sent_index)/ambiguity), filter(d2,reduced=="reduced", LSTM == "JRNN"))
summary(mj.reducedonly)

mg.reducedonly = aov(surprisal ~ ambiguity + Error(as.factor(sent_index)/ambiguity), filter(d2,reduced=="reduced", LSTM == "GRNN"))
summary(mg.reducedonly)

m.unreducedonly = aov(surprisal ~ ambiguity + Error(as.factor(sent_index)/ambiguity), filter(d2,reduced=="unreduced"))
summary(m.unreducedonly)

mj.unambigonly = aov(surprisal ~ reduced + Error(as.factor(sent_index)/reduced), filter(d2, LSTM == "JRNN", ambiguity=="unambig"))
summary(mj.unambigonly)

mg.unambigonly = aov(surprisal ~ reduced + Error(as.factor(sent_index)/reduced), filter(d2, LSTM == "GRNN", ambiguity=="unambig"))
summary(mg.unambigonly)

m.ambigonly = aov(surprisal ~ reduced + Error(as.factor(sent_index)/reduced), filter(d2,ambiguity=="ambig"))
summary(m.ambigonly)
```


## Surprisal in the end region: main effect of reduction, no interaction with ambiguity
```{r}
d2 = d_agg %>% 
 filter(region == "End")

mj = lmer(surprisal ~ ambiguity + reduced + (1|sent_index), data=filter(d2, LSTM == "JRNN")) # maximal converging
mg = lmer(surprisal ~ ambiguity + reduced + (1|sent_index), data=filter(d2, LSTM == "GRNN")) # maximal converging
mji = lmer(surprisal ~ ambiguity * reduced + (1|sent_index), data=filter(d2, LSTM == "JRNN")) # maximal converging
mgi = lmer(surprisal ~ ambiguity * reduced + (1|sent_index), data=filter(d2, LSTM == "GRNN")) # maximal converging

summary(mj)
summary(mg)
summary(mji)
summary(mgi)

```

End region has main effect of reduction, but no interactions with ambiguity.

# Effects of frequency

The main effect of reducedness on verb surprisal suggests that even an unambiguous verb like "given" only serves as a weak or noisy cue that the following material is a relative clause. This raises the question: what factors influence the noisiness of the verb as a cue? Here we test the hypothesis that more frequent verbs are stronger cues for RC-hood in the neural representation. The reasoning is: the network has had more opportunities to see this verb introducing an RC, and its syntactic representations are strongest for lexical distributions that match its training distribution. If this is truly a property of neural representations, it is something that is not (strongly) shared with humans.

``` {r}
# Read in Google Web N-Gram frequency data

freq = read_csv("../../data/vocab.csv") %>% select(-X1)
df = left_join(d, freq)

# Get the RC verb frequency for each item and condition.
rc_verb_frequency = df %>% 
  filter(region == "RC Verb") %>%
  select(sent_index, ambiguity, reduced, frequency, word) %>%
  rename(rc_verb_frequency=frequency) %>%
  mutate(log_rc_verb_frequency=log(rc_verb_frequency))

df_agg = d_agg %>%
  rename_LSTM() %>%
  inner_join(rc_verb_frequency)
```

### Unambiguous verbs

First, let's just look at the case of an unambiguous verb, and see if the surprisal at the disambiguator is a function of the frequency of the reduced relative clause.

```{r}
df_agg %>%
  filter(region == "Disambiguator",
         ambiguity == "unambig") %>%
  ggplot(aes(x=log_rc_verb_frequency, y=surprisal, color=reduced, label=word)) +
    geom_text() +
    xlab("RC verb log frequency") +
    ylab("Disambiguating verb surprisal") +
    facet_wrap(~LSTM)
```

One way to read this is that as RC verb gets more frequent, there is less and less of a difference between the reduced and unreduced cases. Let's plot that explicitly.

```{r}
df_agg %>%
  distinct() %>%
  filter(region == "Disambiguator",
         ambiguity == "unambig") %>%
  spread(reduced, surprisal) %>%
  mutate(surprisal_diff=reduced - unreduced) %>%
  ggplot(aes(x=log_rc_verb_frequency, y=surprisal_diff, label=word)) +
    geom_text() +
    xlab("RC verb log frequency") +
    ylab("(reduced - unreduced) matrix verb surprisal") +
    stat_smooth(method='lm') +
    facet_wrap(~LSTM)
```

This shows that for unambiguous verbs, as the verb frequency increases, the effect of reduction decreases. That is, more frequent unambiguous verbs are treated more and more like reduced RCs! At least in JRNN. GRNN doesn't show this effect.

Let's generate the JRNN figure:

```{r}
df_agg %>%
  distinct() %>%
  filter(region == "Disambiguator",
         ambiguity == "unambig",
         LSTM == "JRNN") %>%
  spread(reduced, surprisal) %>%
  mutate(surprisal_diff=reduced - unreduced) %>%
  ggplot(aes(x=log_rc_verb_frequency, y=surprisal_diff, label=word)) +
    geom_text() +
    xlab("RC verb log frequency") +
    ylab("(reduced - unreduced) matrix verb surprisal") +
    stat_smooth(method='lm')

ggsave("frequency-garden-path.pdf", width=5, height=5)
```

What's the correlation in that figure?

```{r}
df_agg %>%
  distinct() %>%
  filter(region == "Disambiguator",
         ambiguity == "unambig",
         LSTM == "JRNN") %>%
  spread(reduced, surprisal) %>%
  mutate(surprisal_diff=reduced - unreduced) %>%
  with(cor.test(surprisal_diff, log_rc_verb_frequency))

```
